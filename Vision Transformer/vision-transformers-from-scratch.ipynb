{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"900e1593-cb3b-4a85-963f-e3447b71a193","_cell_guid":"9babe843-9594-4762-9390-4298bb0c8ab3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-06T11:59:01.270303Z","iopub.execute_input":"2025-09-06T11:59:01.270581Z","iopub.status.idle":"2025-09-06T11:59:04.292919Z","shell.execute_reply.started":"2025-09-06T11:59:01.270558Z","shell.execute_reply":"2025-09-06T11:59:04.291724Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport numpy as np","metadata":{"_uuid":"f135ba37-9823-4a03-b99c-c1dc2dbe769d","_cell_guid":"a82334d6-643b-4b61-a922-cabc47faeeed","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:38.461074Z","iopub.execute_input":"2025-09-07T09:26:38.461347Z","iopub.status.idle":"2025-09-07T09:26:46.968350Z","shell.execute_reply.started":"2025-09-07T09:26:38.461328Z","shell.execute_reply":"2025-09-07T09:26:46.967411Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"efedc33e-1715-4077-a205-ec6da1106b97","_cell_guid":"f507ba72-240d-4e7f-9f44-f4f2cc421401","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:46.969871Z","iopub.execute_input":"2025-09-07T09:26:46.970304Z","iopub.status.idle":"2025-09-07T09:26:48.405475Z","shell.execute_reply.started":"2025-09-07T09:26:46.970273Z","shell.execute_reply":"2025-09-07T09:26:48.404720Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/nerminnuraydogan/vision-transformer","metadata":{"_uuid":"3339cbc5-b593-4fba-b798-4951a32122f3","_cell_guid":"d9fe71ab-482d-4a4a-8d26-b43ccc8972e4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:48.406236Z","iopub.execute_input":"2025-09-07T09:26:48.406601Z","iopub.status.idle":"2025-09-07T09:26:49.597221Z","shell.execute_reply.started":"2025-09-07T09:26:48.406544Z","shell.execute_reply":"2025-09-07T09:26:49.596075Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = Image.open('vision-transformer/car.png')","metadata":{"_uuid":"048eaf04-4a31-4ae7-8363-6d86c57ce59c","_cell_guid":"c9683d6f-a408-43e2-8b03-4cd97d144ccc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:49.599881Z","iopub.execute_input":"2025-09-07T09:26:49.600175Z","iopub.status.idle":"2025-09-07T09:26:49.653119Z","shell.execute_reply.started":"2025-09-07T09:26:49.600147Z","shell.execute_reply":"2025-09-07T09:26:49.651804Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = image.resize((128, 128))\n\n# convert to numpy array \nx = np.array(image)\n\n\n# An Image Is Worth 16x16 Words\nP = 16   # patch size\nC = 3    # number of channels (RGB)\n\n# split image into patches using numpy\npatches = x.reshape(x.shape[0]//P, P, x.shape[1]//P, P, C).swapaxes(1, 2).reshape(-1, P, P, C)\n\n# flatten patches\nx_p = np.reshape(patches, (-1, P * P * C))\n\n# get number of patches\nN = x_p.shape[0]\n\nprint('Image shape: ', x.shape)  # width, height, channel\nprint('Number of patches: {} with resolution ({}, {})'.format(N, P, P))\nprint('Patches shape: ', patches.shape)\nprint('Flattened patches shape: ', x_p.shape)","metadata":{"_uuid":"dc1f4a4a-f067-4076-a9e2-dfdb23b0806a","_cell_guid":"987a3234-787e-478a-bb81-15b01ceade2a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:49.654004Z","iopub.execute_input":"2025-09-07T09:26:49.654259Z","iopub.status.idle":"2025-09-07T09:26:49.703220Z","shell.execute_reply.started":"2025-09-07T09:26:49.654238Z","shell.execute_reply":"2025-09-07T09:26:49.702401Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure()\n\ngridspec = fig.add_gridspec(1, 2)\nax1 = fig.add_subplot(gridspec[0])\nax1.set(title='Image')\n\n# display image \nax1.imshow(x)\n\nsubgridspec = gridspec[1].subgridspec(8, 8, hspace=-0.8)\n\n# display patches\nfor i in range(8):    # N = 64, 8x8 grid\n    for j in range(8):\n        num = i * 8 + j\n        ax = fig.add_subplot(subgridspec[i, j])\n        ax.set(xticks=[], yticks=[])\n        ax.imshow(patches[num])","metadata":{"_uuid":"a3b16d12-ac43-4265-be18-b93524f4c621","_cell_guid":"15aad4fd-141d-4e26-8803-68bf0fb1ebb9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:49.704052Z","iopub.execute_input":"2025-09-07T09:26:49.704334Z","iopub.status.idle":"2025-09-07T09:26:52.209386Z","shell.execute_reply.started":"2025-09-07T09:26:49.704307Z","shell.execute_reply":"2025-09-07T09:26:52.208339Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"D = 768\n\n# batch size\nB = 1\n\n# convert flattened patches to tensor\nx_p = torch.Tensor(x_p)\n\n# add batch dimension\nx_p = x_p[None, ...]    \n\n# weight matrix E\nE = nn.Parameter(torch.randn(1, P * P * C, D))\n\npatch_embeddings = torch.matmul(x_p , E)\n\nassert patch_embeddings.shape == (B, N, D)\nprint(patch_embeddings.shape)","metadata":{"_uuid":"5cabae00-cbeb-4c76-a102-65bf534decc4","_cell_guid":"d9da0111-de2d-4077-9ad3-f49b8b9870d2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:52.210487Z","iopub.execute_input":"2025-09-07T09:26:52.210846Z","iopub.status.idle":"2025-09-07T09:26:52.249161Z","shell.execute_reply.started":"2025-09-07T09:26:52.210807Z","shell.execute_reply":"2025-09-07T09:26:52.248316Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Class Token","metadata":{"_uuid":"bd8f97fc-19ee-400a-8a95-9865675e6b0d","_cell_guid":"511afb54-5d39-4f52-8e76-c4e7c0361d0d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# init class token\nclass_token = nn.Parameter(torch.randn(1, 1, D))\n\npatch_embeddings = torch.cat((class_token, patch_embeddings), 1)\n\nprint(patch_embeddings.shape)\nassert patch_embeddings.shape == (B, N + 1, D)","metadata":{"_uuid":"a492f53e-626d-4477-95a4-524744ea07f2","_cell_guid":"c87a299a-9902-4f2c-a097-4e356e225445","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:52.250137Z","iopub.execute_input":"2025-09-07T09:26:52.250601Z","iopub.status.idle":"2025-09-07T09:26:52.261610Z","shell.execute_reply.started":"2025-09-07T09:26:52.250549Z","shell.execute_reply":"2025-09-07T09:26:52.260750Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Position Embedding","metadata":{"_uuid":"4d99a706-4fb8-4fad-a8a7-647ddffd236f","_cell_guid":"40ad71cc-6ccf-4ff3-83cd-45a20f39d2aa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# position embeddings\nE_pos = nn.Parameter(torch.randn(1, N + 1, D))\n\nz0 = patch_embeddings + E_pos\n\nprint(z0.shape)\nassert z0.shape == (B, N + 1, D)","metadata":{"_uuid":"5b1e8145-1e00-4adb-903c-716a7eb92253","_cell_guid":"56f43db0-b64e-4e98-afcd-495fca89caee","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:52.262521Z","iopub.execute_input":"2025-09-07T09:26:52.262840Z","iopub.status.idle":"2025-09-07T09:26:52.277484Z","shell.execute_reply.started":"2025-09-07T09:26:52.262816Z","shell.execute_reply":"2025-09-07T09:26:52.276546Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Self Attention","metadata":{"_uuid":"fa64c449-d75f-423b-b405-549c26966f04","_cell_guid":"a20f899c-1836-4ab9-b032-1f427a267ece","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, embedding_dim=768, key_dim=64):\n        super(SelfAttention, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.key_dim = key_dim\n\n        self.W_q = nn.Linear(embedding_dim, key_dim, bias=False)\n        self.W_k = nn.Linear(embedding_dim, key_dim, bias=False)\n        self.W_v = nn.Linear(embedding_dim, key_dim, bias=False)\n\n    def forward(self, x):\n        key_dim = self.key_dim\n\n        q = self.W_q(x)   # (B, N, d_k)\n        k = self.W_k(x)   # (B, N, d_k)\n        v = self.W_v(x)   # (B, N, d_k)\n\n        sims = torch.matmul(q, k.transpose(-2, -1))   # (B, N, N)\n        scaled_sims = sims / np.sqrt(key_dim)\n        \n        attention_weights = F.softmax(scaled_sims, dim=-1)  # normalize over keys\n\n        weighted_values = torch.matmul(attention_weights, v)  # (B, N, d_k)\n        return weighted_values","metadata":{"_uuid":"e93bb244-2282-49f4-8f56-8da07f94eb23","_cell_guid":"0018a484-8ff8-4b21-b872-d5752be4add9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:52.280177Z","iopub.execute_input":"2025-09-07T09:26:52.280479Z","iopub.status.idle":"2025-09-07T09:26:52.294014Z","shell.execute_reply.started":"2025-09-07T09:26:52.280458Z","shell.execute_reply":"2025-09-07T09:26:52.293105Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"D_h = 64\n\n# init self-attention\nself_attention = SelfAttention(D, D_h)   # embedding_dim, key_dim\n\nattention_scores = self_attention(patch_embeddings)\n\nprint(attention_scores.shape)\nassert attention_scores.shape == (B, N + 1, D_h)","metadata":{"_uuid":"dbec6b64-6fbe-44e9-9c7c-a8918020f313","_cell_guid":"204cb195-58b9-4f0b-be8c-d137b685ae47","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:26:52.294985Z","iopub.execute_input":"2025-09-07T09:26:52.295285Z","iopub.status.idle":"2025-09-07T09:26:52.331414Z","shell.execute_reply.started":"2025-09-07T09:26:52.295262Z","shell.execute_reply":"2025-09-07T09:26:52.330527Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multi-Head Self-Attention","metadata":{"_uuid":"4dbe4ec7-bf4f-4263-b46e-8c853768212d","_cell_guid":"b3a806db-2395-4631-837f-9349d8420b29","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"num_heads = 12            # set number of heads (k)\nembedding_dim = 768    # set dimensionality\n\nassert embedding_dim % num_heads == 0   # dimensionality should be divisible by number of heads\nkey_dim = embedding_dim // num_heads   # set key,query and value dimensionality\n\n        # init self-attentions\nattention_list = [SelfAttention(embedding_dim, key_dim) for _ in range(num_heads)]\nmulti_head_attention = nn.ModuleList(attention_list)\n\n        # init U_msa weight matrix\nW = nn.Parameter(torch.randn(num_heads * key_dim, embedding_dim))","metadata":{"_uuid":"a331ea00-689e-4511-b3ca-342cb0b07408","_cell_guid":"408b7dae-6f24-45af-9e6e-34c61193a5e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:35:49.896068Z","iopub.execute_input":"2025-09-07T09:35:49.897023Z","iopub.status.idle":"2025-09-07T09:35:49.951067Z","shell.execute_reply.started":"2025-09-07T09:35:49.896986Z","shell.execute_reply":"2025-09-07T09:35:49.949902Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attention_scores = [attention(patch_embeddings) for attention in multi_head_attention]\nfor i in attention_scores:\n    print(i.shape)","metadata":{"_uuid":"04dfe62b-8bc7-4e2e-b704-34c5607958d9","_cell_guid":"cd6db779-6611-4982-8a3d-dc67e14e7d81","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:37:45.421132Z","iopub.execute_input":"2025-09-07T09:37:45.421432Z","iopub.status.idle":"2025-09-07T09:37:45.436647Z","shell.execute_reply.started":"2025-09-07T09:37:45.421411Z","shell.execute_reply":"2025-09-07T09:37:45.435692Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Z = torch.cat(attention_scores, -1)\nprint(Z.shape)\n\nprint(W.shape)\n\nattention_score = torch.matmul(Z, W)\nprint(attention_score.shape)","metadata":{"_uuid":"e09bfbc3-f88c-413c-b75d-c1433753f372","_cell_guid":"ff0a03db-0dee-40bf-b489-5b5a57b1196e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:40:41.032029Z","iopub.execute_input":"2025-09-07T09:40:41.032815Z","iopub.status.idle":"2025-09-07T09:40:41.039288Z","shell.execute_reply.started":"2025-09-07T09:40:41.032788Z","shell.execute_reply":"2025-09-07T09:40:41.038436Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embedding_dim=768, num_heads=12):\n        super(MultiHeadSelfAttention, self).__init__()\n\n        self.num_heads = num_heads            # set number of heads (k)\n        self.embedding_dim = embedding_dim    # set dimensionality\n\n        assert embedding_dim % num_heads == 0   # dimensionality should be divisible by number of heads\n        self.key_dim = embedding_dim // num_heads   # set key,query and value dimensionality\n\n        # init self-attentions\n        self.attention_list = [SelfAttention(embedding_dim, self.key_dim) for _ in range(num_heads)]\n        self.multi_head_attention = nn.ModuleList(self.attention_list)\n\n        # init U_msa weight matrix\n        self.W = nn.Parameter(torch.randn(num_heads * self.key_dim, embedding_dim))\n\n    def forward(self, x):\n        # compute self-attention scores of each head\n        attention_scores = [attention(x) for attention in self.multi_head_attention]\n\n        # concat attentions\n        Z = torch.cat(attention_scores, -1)\n\n        # compute multi-head attention score\n        attention_score = torch.matmul(Z, self.W)\n\n        return attention_score","metadata":{"_uuid":"cec0b4fd-430d-406f-a8d5-c3c261f9837e","_cell_guid":"f7d204fc-11d3-40b2-8ef4-f1200235398c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:54:05.601549Z","iopub.execute_input":"2025-09-07T09:54:05.602386Z","iopub.status.idle":"2025-09-07T09:54:05.610267Z","shell.execute_reply.started":"2025-09-07T09:54:05.602353Z","shell.execute_reply":"2025-09-07T09:54:05.608870Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multi-Layer Perceptron","metadata":{"_uuid":"54e0a224-4b9e-4f34-a649-fa51f742c255","_cell_guid":"8d741557-6520-4fd5-911e-71385ca1e890","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class MultiLayerPerceptron(nn.Module):\n    def __init__(self, embedding_dim=768, hidden_dim=3072):\n        super(MultiLayerPerceptron, self).__init__()\n\n        self.mlp = nn.Sequential(\n                            nn.Linear(embedding_dim, hidden_dim),\n                            nn.GELU(),\n                            nn.Linear(hidden_dim, hidden_dim),\n                            nn.GELU(),\n                            nn.Linear(hidden_dim, embedding_dim)\n                   )\n\n    def forward(self, x):\n        # pass through multi-layer perceptron\n        x = self.mlp(x)\n        return x","metadata":{"_uuid":"cd517123-1f05-40b9-86bb-951b675eedc1","_cell_guid":"1e3e931c-38a4-484a-a95c-fc99ec042092","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:53:51.015176Z","iopub.execute_input":"2025-09-07T09:53:51.016265Z","iopub.status.idle":"2025-09-07T09:53:51.022216Z","shell.execute_reply.started":"2025-09-07T09:53:51.016224Z","shell.execute_reply":"2025-09-07T09:53:51.021205Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hidden_dim = 3072\n\n# init mlp\nmlp = MultiLayerPerceptron(D, hidden_dim)\n\n# compute mlp output\noutput = mlp(patch_embeddings)\n\nassert output.shape == (B, N + 1, D)\noutput.shape","metadata":{"_uuid":"d3336881-c083-4607-9713-d8f08ee72383","_cell_guid":"43cbed06-c7f8-4b37-9a5d-9bb78376f3d8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T09:53:52.827227Z","iopub.execute_input":"2025-09-07T09:53:52.827654Z","iopub.status.idle":"2025-09-07T09:53:53.043528Z","shell.execute_reply.started":"2025-09-07T09:53:52.827620Z","shell.execute_reply":"2025-09-07T09:53:53.042555Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transformer Encoder","metadata":{"_uuid":"74c844e3-e6ad-43ef-a6b7-ec8df419bf8c","_cell_guid":"8ec06258-0eac-4cc6-8f77-3126b6caa8ce","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self, embedding_dim=768, num_heads=12, hidden_dim=3072, dropout_prob=0.1):\n        super().__init__()\n\n        self.MSA = MultiHeadSelfAttention(embedding_dim, num_heads)\n        self.MLP = MultiLayerPerceptron(embedding_dim, hidden_dim)\n\n        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n\n        self.dropout1 = nn.Dropout(dropout_prob)\n        self.dropout2 = nn.Dropout(dropout_prob)\n\n    def forward(self, x):\n        # --- Multi-head self-attention ---\n        norm_x = self.layer_norm1(x)\n        msa_out = self.MSA(norm_x)\n        msa_out = self.dropout1(msa_out)\n        x = x + msa_out   # residual connection\n\n        # --- Feed-forward network ---\n        norm_x = self.layer_norm2(x)\n        mlp_out = self.MLP(norm_x)\n        mlp_out = self.dropout2(mlp_out)\n        x = x + mlp_out   # residual connection\n\n        return x","metadata":{"_uuid":"1e7a5b88-d2d1-4fca-8ee7-a6b9167c83a1","_cell_guid":"ffc6edd5-9bbb-40d2-ae50-a4a8e5f568fb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T10:20:40.987506Z","iopub.execute_input":"2025-09-07T10:20:40.987942Z","iopub.status.idle":"2025-09-07T10:20:40.994903Z","shell.execute_reply.started":"2025-09-07T10:20:40.987916Z","shell.execute_reply":"2025-09-07T10:20:40.993983Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dropout_prob = 0.1\n\n# init transformer encoder\ntransformer_encoder = TransformerEncoder(D, n_head, hidden_dim, dropout_prob)\n\n# compute transformer encoder output\noutput = transformer_encoder(patch_embeddings)\n\nassert output.shape == (B, N + 1, D)\noutput.shape","metadata":{"_uuid":"fedecf2b-cd67-4a59-8b0a-8b448a49ae8b","_cell_guid":"ce3e29e8-7555-40ae-9897-347ccbd0b116","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T10:21:02.571691Z","iopub.execute_input":"2025-09-07T10:21:02.572025Z","iopub.status.idle":"2025-09-07T10:21:02.805442Z","shell.execute_reply.started":"2025-09-07T10:21:02.572004Z","shell.execute_reply":"2025-09-07T10:21:02.804544Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MLP Head","metadata":{"_uuid":"52f12c28-bba4-4071-b186-28a18111c22b","_cell_guid":"ac5134e9-ccad-40a3-86f9-cc32cddb16a7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class MLPHead(nn.Module):\n    def __init__(self, embedding_dim=768, num_classes=10, fine_tune=False):\n        super(MLPHead, self).__init__()\n        self.num_classes = num_classes\n\n        if not fine_tune:\n            # hidden layer with tanh activation function\n            self.mlp_head = nn.Sequential(\n                                    nn.Linear(embedding_dim, 3072),  # hidden layer\n                                    nn.Tanh(),\n                                    nn.Linear(3072, num_classes)    # output layer\n                            )\n        else:\n            # single linear layer\n            self.mlp_head = nn.Linear(embedding_dim, num_classes)\n\n    def forward(self, x):\n        x = self.mlp_head(x)\n        return x","metadata":{"_uuid":"39912187-ab08-44c8-95c5-17634a79cef9","_cell_guid":"7eed0ba5-5fc9-4a20-b5f2-fbc2a8d95bd9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T10:21:50.399135Z","iopub.execute_input":"2025-09-07T10:21:50.399939Z","iopub.status.idle":"2025-09-07T10:21:50.406177Z","shell.execute_reply.started":"2025-09-07T10:21:50.399907Z","shell.execute_reply":"2025-09-07T10:21:50.405290Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cls_token = output[0][0]\n\nn_classes = 10\n\nmlp_head_pretrain = MLPHead(D, n_class)\n\noutput_1 = mlp_head_pretrain(z_L)\noutput_1","metadata":{"_uuid":"9ddabec0-d3a0-49dd-8e83-866ba0602ea6","_cell_guid":"b65ef519-90ad-4e50-931b-8eb59bcc0e39","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T10:26:38.835153Z","iopub.execute_input":"2025-09-07T10:26:38.835434Z","iopub.status.idle":"2025-09-07T10:26:38.872090Z","shell.execute_reply.started":"2025-09-07T10:26:38.835415Z","shell.execute_reply":"2025-09-07T10:26:38.871082Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"F.softmax(output_1, dim=0)","metadata":{"_uuid":"c7671ac6-0638-4788-a89a-bd3624c203e8","_cell_guid":"cf1e030b-49e5-4c75-bac3-87843abd70ca","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-07T10:29:18.748147Z","iopub.execute_input":"2025-09-07T10:29:18.748443Z","iopub.status.idle":"2025-09-07T10:29:18.756601Z","shell.execute_reply.started":"2025-09-07T10:29:18.748423Z","shell.execute_reply":"2025-09-07T10:29:18.755423Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    def __init__(self, patch_size=16, image_size=224, channel_size=3,\n                     num_layers=1, embedding_dim=768, num_heads=12, hidden_dim=3072,\n                            dropout_prob=0.1, num_classes=10, pretrain=True):\n        super(VisionTransformer, self).__init__()\n\n        self.patch_size = patch_size\n        self.channel_size = channel_size\n        self.num_layers = num_layers\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.dropout_prob = dropout_prob\n        self.num_classes = num_classes\n\n        # get number of patches of the image\n        self.num_patches = int(image_size ** 2 / patch_size ** 2)   # height * width / patch size ^ 2\n\n        # trainable linear projection for mapping dimension of patches (weight matrix E)\n        self.W = nn.Parameter(\n                    torch.randn( patch_size * patch_size * channel_size, embedding_dim))\n\n        # position embeddings (E_pos)\n        self.pos_embedding = nn.Parameter(torch.randn(self.num_patches + 1, embedding_dim))\n\n        # learnable class token embedding (x_class)\n        self.class_token = nn.Parameter(torch.rand(1, D))\n\n        # stack transformer encoder layers\n        transformer_encoder_list = [\n            TransformerEncoder(embedding_dim, num_heads, hidden_dim, dropout_prob)\n                    for _ in range(num_layers)]\n        self.transformer_encoder_layers = nn.Sequential(*transformer_encoder_list)\n\n        # mlp head\n        self.mlp_head = MLPHead(embedding_dim, num_classes)\n\n    def forward(self, x):\n        # get patch size and channel size\n        P, C = self.patch_size, self.channel_size\n\n        # split image into patches\n        patches = x.unfold(1, C, C).unfold(2, P, P).unfold(3, P, P)\n        patches = patches.contiguous().view(patches.size(0), -1, C * P * P).float()\n\n        # linearly embed patches\n        patch_embeddings = torch.matmul(patches , self.W)\n\n        # add class token\n        batch_size = patch_embeddings.shape[0]\n        patch_embeddings = torch.cat((self.class_token.repeat(batch_size, 1, 1), patch_embeddings), 1)\n\n        # add positional embedding\n        patch_embeddings = patch_embeddings + self.pos_embedding\n\n        # feed patch embeddings into a stack of Transformer encoders\n        transformer_encoder_output = self.transformer_encoder_layers(patch_embeddings)\n\n        # extract [class] token from encoder output\n        output_class_token = transformer_encoder_output[:, 0]\n\n        # pass token through mlp head for classification\n        y = self.mlp_head(output_class_token)\n\n        return y","metadata":{"_uuid":"f2e3fd4d-f0d3-4e18-b689-4bf3922285e6","_cell_guid":"8fa9ae88-0659-4e40-9fba-d4e3db6974fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-07T11:11:11.571358Z","iopub.execute_input":"2025-09-07T11:11:11.571695Z","iopub.status.idle":"2025-09-07T11:11:11.583266Z","shell.execute_reply.started":"2025-09-07T11:11:11.571672Z","shell.execute_reply":"2025-09-07T11:11:11.582087Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"model = VisionTransformer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T11:11:12.168306Z","iopub.execute_input":"2025-09-07T11:11:12.169247Z","iopub.status.idle":"2025-09-07T11:11:12.399733Z","shell.execute_reply.started":"2025-09-07T11:11:12.169217Z","shell.execute_reply":"2025-09-07T11:11:12.398803Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"model.to(\"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T11:11:14.220790Z","iopub.execute_input":"2025-09-07T11:11:14.221846Z","iopub.status.idle":"2025-09-07T11:11:14.230091Z","shell.execute_reply.started":"2025-09-07T11:11:14.221807Z","shell.execute_reply":"2025-09-07T11:11:14.229145Z"}},"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (transformer_encoder_layers): Sequential(\n    (0): TransformerEncoder(\n      (MSA): MultiHeadSelfAttention(\n        (multi_head_attention): ModuleList(\n          (0-11): 12 x SelfAttention(\n            (W_q): Linear(in_features=768, out_features=64, bias=False)\n            (W_k): Linear(in_features=768, out_features=64, bias=False)\n            (W_v): Linear(in_features=768, out_features=64, bias=False)\n          )\n        )\n      )\n      (MLP): MultiLayerPerceptron(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=3072, out_features=3072, bias=True)\n          (3): GELU(approximate='none')\n          (4): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (dropout2): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (mlp_head): MLPHead(\n    (mlp_head): Sequential(\n      (0): Linear(in_features=768, out_features=3072, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=3072, out_features=10, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":100},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}